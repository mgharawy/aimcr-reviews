{
  "metadata": {
    "proposal_title": "Scaling Laws and Optimization Dynamics of Looped Transformers: Toward Efficient and High-Capacity Language Models",
    "principal_investigator": "Di Wang",
    "proposal_date": "2026-01-11",
    "reviewer_name": "Mohsin Ahmed Shaikh",
    "reviewer_id": "174988",
    "aimcr_date": "2026-01-11",
    "project_id": "65594"
  },
  "third_party_software": [
    {
      "name": "PyTorch",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Required to implement and run transformer-based LLM training experiments (including distributed training with Megatron-LM) to study Looped Transformer scaling laws and optimization dynamics. Aligns with the approved topic, Data Science and Engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No indication of prohibited or restricted use (including military, weapons, surveillance, or intelligence functionality) is stated or implied in the PyTorch 2.9.0 documentation, which describes a general-purpose open-source machine learning framework; this review was conducted in accordance with LC 2.7."
        },
        {
          "name": "Restricted Entities Screening (LC 2.5)",
          "score": 1,
          "notes": "No indication of restricted-country origin or restricted-entity involvement was identified for Torchtitan v0.2.0 based on available documentation, contributor information, and repository metadata.\\nThis review was conducted in accordance with LC 2.5."
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "3.1 Source Channel\nGitHub: https://github.com/pytorch/pytorch\n3.2 GitHub Repository Metadata\nOwner / Organization: PyTorch Foundation\nRepository: pytorch/pytorch\nDescription: Tensors and Dynamic neural networks in Python with strong GPU acceleration\nStars: 96510\nForks: 26483\nOpen Issues: 17875\nLast Commit Date: 2026-01-11T03:50:11Z\nActivity Health: Healthy"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "4.1 License Type\nSPDX ID: N/A\nLicense name: BSD 3-Clause\n4.2 Permissibility Summary\nQuestion\tAnswer\nCan be used for academic purposes?\tYes\nCan be used for commercial use?\tYes\nModification allowed?\tYes\nRedistribution allowed?\tYes\nAttribution required?\tYes\nCopyleft obligations?\tNo"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "torch 2.9.0\nnvidia-cublas-cu12 12.8.4.1\nnvidia-cuda-cupti-cu12 12.8.90\nnvidia-cuda-nvrtc-cu12 12.8.93\nnvidia-cuda-runtime-cu12 12.8.90\nnvidia-cudnn-cu12 9.10.2.21\nnvidia-cufft-cu12 11.3.3.83\nnvidia-cufile-cu12 1.13.1.3\nnvidia-curand-cu12 10.3.9.90\nnvidia-cusolver-cu12 11.7.3.90\nnvidia-cusparse-cu12 12.5.8.93\nnvidia-cusparselt-cu12 0.7.1\nnvidia-nccl-cu12 2.27.5\nnvidia-nvjitlink-cu12 12.8.93\nnvidia-nvshmem-cu12 3.3.20\nnvidia-nvtx-cu12 12.8.90\ntriton 3.5.0\nfsspec 2026.1.0\nnetworkx 3.6.1\nsympy 1.14.0\nmpmath 1.3.0\ntyping_extensions 4.15.0\nfilelock 3.20.3\nJinja2 3.1.6\nMarkupSafe 3.0.3\nsetuptools 80.9.0\n"
        }
      ]
    },
    {
      "name": "Megatron-LM (0.15.0)",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Required to implement and scale large transformer model pretraining to support the project\u2019s objectives of analyzing and optimizing LLM training dynamics. Aligns with the approved topic, Data Science and Engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No indication of prohibited or restricted use (including military, weapons, surveillance, or intelligence functionality) is stated or implied in the Megatron-LM v0.15.0 documentation, which describes a research-oriented large-scale language model training framework; this review was conducted in accordance with LC 2.7."
        },
        {
          "name": "Restricted Entities Screening (LC 2.5)",
          "score": 1,
          "notes": "No indication of restricted-country origin or restricted-entity involvement was identified for Torchtitan v0.2.0 based on available documentation, contributor information, and repository metadata. This review was conducted in accordance with LC 2.5."
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "3.1 Source Channel\nGitHub: https://github.com/NVIDIA/Megatron-LM\n3.2 GitHub Repository Metadata\nOwner / Organization: NVIDIA\nRepository: NVIDIA/Megatron-LM\nDescription: Ongoing research training transformer models at scale\nStars: 14862\nForks: 3478\nOpen Issues: 558\nLast Commit Date: 2026-01-10T07:07:09Z\nActivity Health: Healthy"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": ".1 License Type\nSPDX ID: N/A\nLicense name: Apache 2.0\n4.2 Permissibility Summary\nQuestion\tAnswer\nCan be used for academic purposes?\tYes\nCan be used for commercial use?\tYes\nModification allowed?\tYes\nRedistribution allowed?\tYes\nAttribution required?\tYes\nCopyleft obligations?\tNo"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "megatron-core 0.15.0\nnumpy 1.26.4\npackaging 25.0\ntorch 2.9.1\nnvidia-cublas-cu12 12.8.4.1\nnvidia-cuda-cupti-cu12 12.8.90\nnvidia-cuda-nvrtc-cu12 12.8.93\nnvidia-cuda-runtime-cu12 12.8.90\nnvidia-cudnn-cu12 9.10.2.21\nnvidia-cufft-cu12 11.3.3.83\nnvidia-cufile-cu12 1.13.1.3\nnvidia-curand-cu12 10.3.9.90\nnvidia-cusolver-cu12 11.7.3.90\nnvidia-cusparse-cu12 12.5.8.93\nnvidia-cusparselt-cu12 0.7.1\nnvidia-nccl-cu12 2.27.5\nnvidia-nvjitlink-cu12 12.8.93\nnvidia-nvshmem-cu12 3.3.20\nnvidia-nvtx-cu12 12.8.90\ntriton 3.5.1\nfsspec 2026.1.0\nnetworkx 3.6.1\nsympy 1.14.0\nmpmath 1.3.0\ntyping_extensions 4.15.0\nfilelock 3.20.3\nJinja2 3.1.6\nMarkupSafe 3.0.3\nsetuptools 80.9.0\n"
        }
      ]
    }
  ],
  "source_code": [],
  "datasets_user_files": [
    {
      "name": "FineWeb-Edu",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "It's the only mentioned dataset."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No indication of prohibited or restricted use (including military, weapons, surveillance, or intelligence functionality) is stated or implied in the FineWeb-Edu dataset documentation, which describes the dataset as an educationally focused web-text corpus for language model research and training; this review was conducted in accordance with LC 2.7."
        },
        {
          "name": "Restricted Entities Screening (LC 2.5)",
          "score": 1,
          "notes": "Originating Organization: HuggingFace and Common Crawl both in United States\n"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No files available"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Preview available by HuggingFace\nhttps://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/viewer/default/train?row=0"
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "\nOwner / Curator: HuggingFace / Fine Data\n\nPrimary Distribution Platform: Hugging Face\n\nRepository: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\n\nAccess Method: download via Hugging Face\n\nProvenance:\n\nHugging Face does not claim ownership of the original webpages\nPrimary raw source: Common Crawl\nHugging Face does own and license the compiled dataset artifact i.e., the database resulting from selection, transformation, and organization of the data\n(277k downloads on HF, 1 commit, July 2025).\nMaintenance Status: Actively maintained (~20 commits; last update 2025-12)"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: ODC-BY\nUse the dataset for any purpose (research, commercial, internal, public)\nModify, transform, and build upon the dataset\nAttribution to the dataset authors is required\nIndicate if changes were made"
        }
      ]
    }
  ],
  "models": [
    {
      "name": "Looped Transformer (shared-block Llama/Qwen-style; custom research model) ",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Decoder-only Looped Transformer that reuses several shared Transformer blocks across multiple loop steps (recurrent depth). Trained for autoregressive language modeling to (1) establish scaling laws for loop depth/width/compute and (2) analyze the strong empirical synergy between Looped Transformers and the Muon optimizer. "
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No documentation  provided for screening"
        },
        {
          "name": "Source / Provenance & Restricted Entities (LC 2.5)",
          "score": 1,
          "notes": "No source provided. The model is to be produced during Shaheen III runs; planned open release (GitHub/Hugging Face) \u2014 URL TBD. \nOwners: KAUST: Di Wang (PI), Liangyu Wang. "
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Research artifacts intended for open publication and open-source release. Training framework licensing: Megatron-LM (Apache-2.0). "
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "Pre-training will use the FineWeb-Edu [1] public text corpus (10 B to 350 B tokens). (HuggingFaceFW/fineweb-edu; Lozhkov et al., 2024; DOI: 10.57967/hf/2497; arXiv:2406.17557). "
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "Models are trained from scratch for language modeling; no downstream fine-tuning in the released artifacts (unless added in later follow-up work). "
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "Effective capacity explored spans ~0.1B\u201310B (via loop depth). \n# of tokens in dataset = 350B tokens in total \nMFU = 38% to 41% \nAMP mixed precision: bf16 for forward/backward compute, FP32 master weights & optimizer updates. \n\nTotal FLOPs\u22486\u00d7Neff\u00d7T\nWhere:\n\nT=350B tokens=3.5\u00d710^11\n\n\nNeff (effective capacity explored) spans: 0.1B to 10B params\n\n\u21d2Neff\u2208[1\u00d7108,\u20091\u00d710^10]\n\nResults\n\nLow end (0.1B effective params):\n6\u22c51\u00d710^8\u22c53.5\u00d710^11=2.1\u00d710^20 FLOPs\n\nHigh end (10B effective params):\n6\u22c51\u00d710^10\u22c53.5\u00d710^11=2.1\u00d710^22 FLOPs\n\nSo the expected compute is:\nTotal FLOPs\u22482.1\u00d710^20 to 2.1\u00d710^22\n\nAchieved FLOP/s=MFU\u00d7Peak FLOP/s\n\nUsing the pessimistic MFU = 38%:\nAchieved FLOP/s\u22480.38\u00d7Peak FLOP/s\nAchieved FLOP/s=0.38\u00d7990 TFLOP/s\u2248376.2 TFLOP/s"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Inspection skipped. No source."
        }
      ],
      "is_proprietary": false
    }
  ],
  "observations": "No concrete, named pretrained models or exact model variants are specified (e.g., \u201cLLaMA-2-7B\u201d, \u201cQwen-7B\u201d, \u201cGPT-NeoX-20B\u201d).\n\nFindings\n- The document does not name any specific existing model checkpoints or fixed model sizes to be used.\n- References to \u201cLlama/Qwen style\u201d are architectural examples only, not commitments to specific models.\n- The project instead proposes training new Transformer-based and Looped Transformer models from scratch across a range of sizes and loop depths. \n",
  "recommendation": "Since the proposal is for a looped transformer, the expectation is that the number of parameters will be significantly less then any stacked transformer architecture. However, the number of FLOPs will remain undetermined due to the lack of information about the architecture. \nEven with this gap, the project can progress to grand challenge with requirement of consultation with the applicants and monitoring when in use.\n\nRecommended for grand challenge. ",
  "_submission_history": [
    {
      "timestamp": "2026-01-12T00:28:33.692561",
      "action": "initial_submission"
    }
  ]
}